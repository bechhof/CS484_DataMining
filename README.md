# CS484_DataMining
Team #4


## Guidelines given for presentation 

TEAM TERM PROJECT (TTP)

BASICS

 Teams 1 – 7 (15 minutes) Class Presentations, Slides, and Hard Copy Final

Report (12 pages or less and typed) are due December 6 – 8

 Final presentation requires running WEKA or similar in class

 You can use a very limited number of additional slides (e.g., 3-1 and 3 -2)

 Use only methods learned in class

 Team members should be able to explain everything shown during their team

final presentations.

 Make sure that the core of your projects is at all times finding

a dependency y = f (x) using k-fold cross-validation

CONTENTS for SLIDES:

20 Slides (mostly bullets) (and hard copy for instructor):

- Title, team members, emails, team leader (1)

- Problem y = f(x): Describe / Explain y, f, and x; Relation to Data Mining and

Application / Utility; Challenges; Background (2.1 and 2.2)

- Data Sources, Sampling / 1. Original data distribution / 2. Balanced distribution using

SMOTE or similar), Data Characterization / Feature Extraction, if any,

and Examples (3, 4);

– Project (Modular) Architecture (5)

– Preprocessing (including Normalization) (6 - 7)

– Feature Selection (motivation and criteria / e.g., information gain) and Dimensionality

Reduction (motivation and criteria) (e.g., PCA) (8, 9)

- Experimental Design, Metrics, and Performance Evaluation (10, 11)

– Learning and Evaluation using 5-fold Cross – Validation (CV) (Train, Tune / Validate,

and Test: Folds 1 - 4) (Sequestered data for testing best methods: Fold 5) (12.1 – 12.2)

- MODEL SELECTION (13, 14, 15)

(A) Include box plot diagram (mean and variance) to rank the performance for the

machine learning methods used (see 10 - 11) (Use 4-fold CV (2 folds for train, 1 fold

for tune/validate, and 1 fold for testing) (Final testing on sequestered data using fold 5)

(B) Methods / Algorithms: Decision Tree (DT), back-propagation (BP), Random Forests

(RF), and Ensemble method / AdaBoost.

(C) Details on parameter settings and estimation (e.g., complexity tradeoffs and

pruning, if any, stopping criteria); Freeze best parameter settings for PREDICTION

using sequestered data / fold 5 (see below).

- PREDICTION (16, 17)

Results on sequestered data (including box plot diagram for ranking the machine

learning methods. Compare / rank the results against those obtained for A (see above)

(16, 17)

- Platforms (of your choice): Software and Hardware (18)

- Discussion (+,–, lessons learned) and Conclusions (what and why if you were to

continue with the project) (Effort spent) (19 - 20)


## Guidelines for Nov 8

Progress Reports (slides) for Team Term Projects (about 15 minutes per team)

(1) November 8 and 10 during class
(2) Cover Core Data Mining including k-fold cross-validation
- Model Selection and Prediction
- Parameter Space {Sampling, Feature Selection} x {Methods 1 - 4}
- Train, Tune / Validate, and Rank
- Test on Sequestered Data
- Metrics and Comparative Performance Evaluation
(3) Cover at least a complete run through slides #1 - # 20
(4) Address Heilmeier questions
http://www.design.caltech.edu/erik/Misc/Heilmeier_Questions.html
(5) Outline Work yet to be done
